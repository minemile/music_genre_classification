{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The autoreload extension is already loaded. To reload it, use:\n",
      "  %reload_ext autoreload\n"
     ]
    }
   ],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 2\n",
    "%reload_ext autoreload\n",
    "import librosa\n",
    "import numpy as np\n",
    "import librosa.display\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import classification_report\n",
    "from server.database_wrapper import PostgresqlWrapper\n",
    "from server.utils import Util\n",
    "from feature_extractor import FeatureExtractor\n",
    "import xgboost as xgb\n",
    "from tqdm import tqdm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Song downloading from database\n",
    "db = PostgresqlWrapper(5)\n",
    "data = db.fetch_songs(150, genres=['classical', 'metal', 'blues', 'hiphop', 'disco', 'pop', 'rock'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "data = list()\n",
    "genre_list = list()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "genres=['classical', 'metal', 'blues', 'hiphop', 'disco', 'pop', 'rock']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def getData(where_to, genre_list, genre, range_tuple):\n",
    "    for i in tqdm(range(range_tuple)):\n",
    "        if i < 10:\n",
    "            path = \"../../\" + genre + \"/\" + genre + \".0000\" + str(i) + \".au\"        \n",
    "        else:\n",
    "            path = \"../../\" + genre + \"/\" + genre + \".000\" + str(i) + \".au\"\n",
    "        \n",
    "        song = librosa.load(path)\n",
    "        where_to.append(song[0])\n",
    "        genre_list.append(genre)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 50/50 [00:29<00:00,  1.93it/s]\n",
      "100%|██████████| 50/50 [00:26<00:00,  1.88it/s]\n",
      "100%|██████████| 50/50 [00:33<00:00,  1.58it/s]\n",
      "100%|██████████| 50/50 [00:32<00:00,  1.53it/s]\n",
      "100%|██████████| 50/50 [00:30<00:00,  1.74it/s]\n",
      "100%|██████████| 50/50 [00:30<00:00,  1.84it/s]\n",
      "100%|██████████| 50/50 [00:27<00:00,  1.86it/s]\n"
     ]
    }
   ],
   "source": [
    "for genre in genres:\n",
    "    getData(data, genre_list, genre, 50)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create dataset\n",
    "util = Util()\n",
    "songs, genres = util.to_dataset(data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Got mfcc for 0 songs\n",
      "Got mfcc for 50 songs\n",
      "Got mfcc for 100 songs\n",
      "Got mfcc for 150 songs\n",
      "Got mfcc for 200 songs\n",
      "Got mfcc for 250 songs\n",
      "Got mfcc for 300 songs\n",
      "Got zero_cross_rate for 0 songs\n",
      "Got zero_cross_rate for 50 songs\n",
      "Got zero_cross_rate for 100 songs\n",
      "Got zero_cross_rate for 150 songs\n",
      "Got zero_cross_rate for 200 songs\n",
      "Got zero_cross_rate for 250 songs\n",
      "Got zero_cross_rate for 300 songs\n",
      "Got centroid data for 0 songs\n",
      "Got centroid data for 50 songs\n",
      "Got centroid data for 100 songs\n",
      "Got centroid data for 150 songs\n",
      "Got centroid data for 200 songs\n",
      "Got centroid data for 250 songs\n",
      "Got centroid data for 300 songs\n"
     ]
    }
   ],
   "source": [
    "# Extract features (mfcc means and zero_crossing_rate for now)\n",
    "extractor = FeatureExtractor(data)\n",
    "mfcc_means = extractor.generate_mfcc(n_mfcc=20, sr=22050)\n",
    "zcrs = extractor.generate_zero_crossing_rate(None, None)\n",
    "cent = extractor.generate_centoid_meanstd()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "X = np.hstack((mfcc_means, zcrs, cent))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(350, 24)"
      ]
     },
     "execution_count": 82,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 83,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
       "       1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
       "       1, 1, 1, 1, 1, 1, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4,\n",
       "       4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4,\n",
       "       4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "       0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "       0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 3, 3, 3, 3,\n",
       "       3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3,\n",
       "       3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3,\n",
       "       3, 3, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2,\n",
       "       2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2,\n",
       "       2, 2, 2, 2, 2, 2, 2, 2, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5,\n",
       "       5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5,\n",
       "       5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 6, 6, 6, 6, 6, 6, 6, 6,\n",
       "       6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6,\n",
       "       6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6])"
      ]
     },
     "execution_count": 83,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y = LabelEncoder().fit_transform(genre_list)\n",
    "y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 111,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Prepare data for xgboost. Encode genres. Train/test split\n",
    "y = LabelEncoder().fit_transform(genre_list)\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.3, random_state=42, stratify=y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 112,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "dtrain = xgb.DMatrix(X_train, label=y_train)\n",
    "dtest = xgb.DMatrix(X_test, label=y_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 113,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[0]\ttrain-merror:0.102041\teval-merror:0.342857\n",
      "[1]\ttrain-merror:0.040816\teval-merror:0.295238\n",
      "[2]\ttrain-merror:0.016327\teval-merror:0.285714\n",
      "[3]\ttrain-merror:0.016327\teval-merror:0.247619\n",
      "[4]\ttrain-merror:0.008163\teval-merror:0.247619\n",
      "[5]\ttrain-merror:0\teval-merror:0.247619\n",
      "[6]\ttrain-merror:0\teval-merror:0.247619\n",
      "[7]\ttrain-merror:0\teval-merror:0.247619\n",
      "[8]\ttrain-merror:0\teval-merror:0.266667\n",
      "[9]\ttrain-merror:0\teval-merror:0.257143\n"
     ]
    }
   ],
   "source": [
    "# Xgboost training\n",
    "param = {'objective': \"multi:softmax\", \"num_class\": np.unique(y_train).size}\n",
    "evallist = [(dtrain, 'train'), (dtest, 'eval')]\n",
    "bst = xgb.train(param, dtrain, evals=evallist)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 114,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "             precision    recall  f1-score   support\n",
      "\n",
      "  classical       0.74      0.93      0.82        15\n",
      "      metal       0.94      1.00      0.97        15\n",
      "      blues       0.62      0.67      0.65        15\n",
      "     hiphop       0.89      0.53      0.67        15\n",
      "      disco       0.80      0.80      0.80        15\n",
      "        pop       0.85      0.73      0.79        15\n",
      "       rock       0.47      0.53      0.50        15\n",
      "\n",
      "avg / total       0.76      0.74      0.74       105\n",
      "\n"
     ]
    }
   ],
   "source": [
    "print(classification_report(y_test, bst.predict(dtest), target_names=['classical', 'metal', 'blues', 'hiphop', 'disco', 'pop', 'rock']))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 100,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Pay attention to rock. SICK!"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
